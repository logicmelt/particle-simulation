from pydantic import Field, model_validator, StringConstraints, BaseModel
from typing import Annotated, Any
from collections import Counter
import numpy as np
import datetime
import scipy.stats
import pandas as pd


class DetectorLoc(BaseModel):
    """The location of the detector."""

    latitude: float = Field(..., description="The latitude of the detector.")
    longitude: float = Field(..., description="The longitude of the detector.")


class ResultsIcos(BaseModel):
    """Results of the icos simulation so that we can save them in a file."""

    start_time: datetime.datetime = Field(
        ..., description="Start time covered by the results."
    )
    end_time: datetime.datetime = Field(
        ..., description="End time covered by the results."
    )
    detector_location: DetectorLoc = Field(
        ..., description="The location of the detector."
    )
    detector_type: Annotated[str, StringConstraints(to_lower=True)] = Field(
        ..., description="Type of the detector, virtual or real."
    )
    mean_zenith: float = Field(
        ...,
        description="Mean zenith angle of the particles detected within the time interval.",
    )
    variance_zenith: float = Field(
        ...,
        description="Variance of the zenith angle of the particles detected within the time interval.",
    )
    skewness_zenith: float = Field(
        ...,
        description="Skewness of the zenith angle of the particles detected within the time interval.",
    )
    kurtosis_zenith: float = Field(
        ...,
        description="Kurtosis of the zenith angle of the particles detected within the time interval.",
    )
    mean_azimuth: float = Field(
        ...,
        description="Mean azimuth angle of the particles detected within the time interval.",
    )
    variance_azimuth: float = Field(
        ...,
        description="Variance of the azimuth angle of the particles detected within the time interval.",
    )
    skewness_azimuth: float = Field(
        ...,
        description="Skewness of the azimuth angle of the particles detected within the time interval.",
    )
    kurtosis_azimuth: float = Field(
        ...,
        description="Kurtosis of the azimuth angle of the particles detected within the time interval.",
    )
    multiplicity: float = Field(
        ...,
        description="Average of the number of particles that reached the detector and were generated by the same primary.",
    )
    relative_time: float = Field(
        ...,
        description="Average of the time it took the particle to reach the detector since the start of the EVENT.",
    )
    temperature: list[float] = Field(
        ...,
        description="List of temperatures at the detector location during the time interval.",
    )
    density: list[float] = Field(
        ...,
        description="List of densities at the detector location during the time interval.",
    )
    altitude: list[float] = Field(
        ...,
        description="List of altitudes where the temperatures and densities were measured.",
    )
    n_readings: int = Field(..., description="Number of readings in the time interval.")

    @model_validator(mode="before")
    @classmethod
    def create_data(
        cls,
        data: tuple[
            pd.DataFrame,
            tuple[
                datetime.datetime,
                datetime.datetime,
                float,
                float,
                int,
                list[list[float]],
            ],
        ],
    ) -> dict[str, Any]:
        """Parses the data from an output.csv file to a standard format.

        Args:
            data (tuple[pd.DataFrame, tuple[datetime.datetime, datetime.datetime, float, float]]): A tuple with both the data and
                the extra information. The extra information is the start time, end time, latitude, longitude, density profile day idx
                and altitude[km]/temperature[K]/density[kg/m3] data.

        Returns:
            dict[str, Any]: Data parsed to standardized format.
        """
        output_data: dict[str, Any] = {}
        # Get the start and end time
        output_data["start_time"] = data[1][0]
        output_data["end_time"] = data[1][1]
        # Get the detector location
        output_data["detector_location"] = DetectorLoc(
            latitude=data[1][2], longitude=data[1][3]
        )
        # Get the temperature, density and altitude
        temperature = []
        density = []
        altitude = []
        for item in data[1][-1]:
            temperature.append(item[1])
            density.append(item[2])
            altitude.append(item[0])
        output_data["temperature"] = temperature
        output_data["density"] = density
        output_data["altitude"] = altitude

        # If the dataframe is empty then everything is zero
        if data[0].empty:
            output_data["n_readings"] = 0
            output_data["detector_type"] = "virtual"
            output_data["mean_zenith"] = 0.0
            output_data["mean_azimuth"] = 0.0
            output_data["variance_zenith"] = 0.0
            output_data["variance_azimuth"] = 0.0
            output_data["skewness_zenith"] = 0.0
            output_data["skewness_azimuth"] = 0.0
            output_data["kurtosis_zenith"] = 0.0
            output_data["kurtosis_azimuth"] = 0.0
            output_data["relative_time"] = 0.0
            output_data["multiplicity"] = 0.0
            return output_data

        output_data["n_readings"] = len(data[0]["ParticleID"])
        output_data["detector_type"] = "virtual"

        # Get the mean zenith and azimuth
        output_data["mean_zenith"] = data[0]["theta"].mean()
        output_data["mean_azimuth"] = data[0]["phi"].mean()

        # Get the variance of the zenith and azimuth
        output_data["variance_zenith"] = data[0]["theta"].var()
        output_data["variance_azimuth"] = data[0]["phi"].var()

        # Get the skewness of the zenith and azimuth
        # Use these instead of pandas skewness because here we can control better the parameters
        # In fact, by default the results are not the same, all the pandas estimations are automatically
        # corrected for statistical bias whereas scipy.stats.skew is not (unless you specify the bias=False)
        output_data["skewness_zenith"] = scipy.stats.skew(data[0]["theta"])
        output_data["skewness_azimuth"] = scipy.stats.skew(data[0]["phi"])

        # Get the kurtosis of the zenith and azimuth
        output_data["kurtosis_zenith"] = scipy.stats.kurtosis(data[0]["theta"])
        output_data["kurtosis_azimuth"] = scipy.stats.kurtosis(data[0]["phi"])

        # The relative time
        output_data["relative_time"] = data[0]["time"].mean()

        # And the average multiplicity
        output_data["multiplicity"] = cls.get_multiplicity(data[0])
        return output_data

    @staticmethod
    def get_multiplicity(data: pd.DataFrame) -> np.float64:
        """Get the average multiplicity of the particles from a dictionary created by the simulation (standard output.csv file format).

        Args:
            data (pd.DataFrame): The data from the simulation.

        Returns:
            np.float64: The average multiplicity of the particles.
        """
        # We need two columns: EventID and process_ID
        # If EventID is the same then it was generated by the same primary but we have to check that the process_ID is also the same
        eventid = data["EventID"].to_numpy()
        processid = data["process_ID"]
        # Get the unique process_ID
        unique_process = processid.unique()
        # Now iterate and get the multiplicity of each process
        multiplicity: list[int] = []
        for process in unique_process:
            # Get the indexes of the process
            indexes = np.where(processid == process)[0]
            iter_multi = Counter(eventid[indexes])
            multiplicity.extend(iter_multi.values())

        return np.mean(multiplicity)


class ResultsInflux(BaseModel):
    """Results of the icos simulation for a influx db."""

    start_time: datetime.datetime = Field(
        ..., description="Start time covered by the results."
    )
    end_time: datetime.datetime = Field(
        ..., description="End time covered by the results."
    )
    detector_location: DetectorLoc = Field(
        ..., description="The location of the detector."
    )
    detector_type: Annotated[str, StringConstraints(to_lower=True)] = Field(
        ..., description="Type of the detector, virtual or real."
    )
    mean_zenith: float = Field(
        ...,
        description="Mean zenith angle of the particles detected within the time interval.",
    )
    variance_zenith: float = Field(
        ...,
        description="Variance of the zenith angle of the particles detected within the time interval.",
    )
    skewness_zenith: float = Field(
        ...,
        description="Skewness of the zenith angle of the particles detected within the time interval.",
    )
    kurtosis_zenith: float = Field(
        ...,
        description="Kurtosis of the zenith angle of the particles detected within the time interval.",
    )
    mean_azimuth: float = Field(
        ...,
        description="Mean azimuth angle of the particles detected within the time interval.",
    )
    variance_azimuth: float = Field(
        ...,
        description="Variance of the azimuth angle of the particles detected within the time interval.",
    )
    skewness_azimuth: float = Field(
        ...,
        description="Skewness of the azimuth angle of the particles detected within the time interval.",
    )
    kurtosis_azimuth: float = Field(
        ...,
        description="Kurtosis of the azimuth angle of the particles detected within the time interval.",
    )
    multiplicity: float = Field(
        ...,
        description="Average of the number of particles that reached the detector and were generated by the same primary.",
    )
    relative_time: float = Field(
        ...,
        description="Average of the time it took the particle to reach the detector since the start of the EVENT.",
    )
    density_day_idx: int = Field(
        ...,
        description="Index of the day in the density json that corresponds to the density profile used in the simulation.",
    )
    n_readings: int = Field(..., description="Number of readings in the time interval.")

    @model_validator(mode="before")
    @classmethod
    def create_data(
        cls,
        data: tuple[
            pd.DataFrame,
            tuple[
                datetime.datetime,
                datetime.datetime,
                float,
                float,
                int,
                list[list[float]],
            ],
        ],
    ) -> dict[str, Any]:
        """Parses the data from an output.csv file to a standard format.

        Args:
            data (tuple[pd.DataFrame, tuple[datetime.datetime, datetime.datetime, float, float]]): A tuple with both the data and
                the extra information. The extra information is the start time, end time, latitude, longitude, density profile day idx
                and altitude[km]/temperature[K]/density[kg/m3] data.

        Returns:
            dict[str, Any]: Data parsed to standardized format.
        """
        output_data: dict[str, Any] = {}
        # Get the start and end time
        output_data["start_time"] = data[1][0]
        output_data["end_time"] = data[1][1]
        # Get the detector location
        output_data["detector_location"] = DetectorLoc(
            latitude=data[1][2], longitude=data[1][3]
        )
        # And the density day index
        output_data["density_day_idx"] = data[1][-2]
        # If the dataframe is empty then everything is zero
        if data[0].empty:
            output_data["n_readings"] = 0
            output_data["detector_type"] = "virtual"
            output_data["mean_zenith"] = 0.0
            output_data["mean_azimuth"] = 0.0
            output_data["variance_zenith"] = 0.0
            output_data["variance_azimuth"] = 0.0
            output_data["skewness_zenith"] = 0.0
            output_data["skewness_azimuth"] = 0.0
            output_data["kurtosis_zenith"] = 0.0
            output_data["kurtosis_azimuth"] = 0.0
            output_data["relative_time"] = 0.0
            output_data["multiplicity"] = 0.0
            return output_data

        # Get the rest of the data
        output_data["n_readings"] = len(data[0]["ParticleID"])
        output_data["detector_type"] = "virtual"

        # Get the mean zenith and azimuth
        output_data["mean_zenith"] = data[0]["theta[rad]"].mean()
        output_data["mean_azimuth"] = data[0]["phi[rad]"].mean()

        # Get the variance of the zenith and azimuth
        output_data["variance_zenith"] = data[0]["theta[rad]"].var()
        output_data["variance_azimuth"] = data[0]["phi[rad]"].var()

        # Get the skewness of the zenith and azimuth
        # Use these instead of pandas skewness because here we can control better the parameters
        # In fact, by default the results are not the same, all the pandas estimations are automatically
        # corrected for statistical bias whereas scipy.stats.skew is not (unless you specify the bias=False)
        output_data["skewness_zenith"] = scipy.stats.skew(data[0]["theta[rad]"])
        output_data["skewness_azimuth"] = scipy.stats.skew(data[0]["phi[rad]"])

        # Get the kurtosis of the zenith and azimuth
        output_data["kurtosis_zenith"] = scipy.stats.kurtosis(data[0]["theta[rad]"])
        output_data["kurtosis_azimuth"] = scipy.stats.kurtosis(data[0]["phi[rad]"])

        # The relative time
        output_data["relative_time"] = data[0]["time[s]"].mean()

        # And the average multiplicity
        output_data["multiplicity"] = cls.get_multiplicity(data[0])
        return output_data

    @staticmethod
    def get_multiplicity(data: pd.DataFrame) -> np.float64:
        """Get the average multiplicity of the particles from a dictionary created by the simulation (standard output.csv file format).

        Args:
            data (pd.DataFrame): The data from the simulation.

        Returns:
            np.float64: The average multiplicity of the particles.
        """
        # We need two columns: EventID and process_ID
        # If EventID is the same then it was generated by the same primary but we have to check that the process_ID is also the same
        eventid = data["EventID"].to_numpy()
        processid = data["process_ID"]
        # Get the unique process_ID
        unique_process = processid.unique()
        # Now iterate and get the multiplicity of each process
        multiplicity: list[int] = []
        for process in unique_process:
            # Get the indexes of the process
            indexes = np.where(processid == process)[0]
            iter_multi = Counter(eventid[indexes])
            multiplicity.extend(iter_multi.values())

        return np.mean(multiplicity)
